<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>paper</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 17pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 h4 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 h3 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li:before {counter-increment: c1; content: counter(c1, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 14pt; }
 #l1> li:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l2> li:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: c2 1; }
 #l3> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l3> li:first-child:before {counter-increment: c2 0;  }
 #l4 {padding-left: 0pt;counter-reset: c3 1; }
 #l4> li:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l4> li:first-child:before {counter-increment: c3 0;  }
 #l5 {padding-left: 0pt;counter-reset: c3 1; }
 #l5> li:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l5> li:first-child:before {counter-increment: c3 0;  }
 #l6 {padding-left: 0pt;counter-reset: c3 1; }
 #l6> li:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 10pt; }
 #l6> li:first-child:before {counter-increment: c3 0;  }
 #l7 {padding-left: 0pt;counter-reset: c2 1; }
 #l7> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)" "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l7> li:first-child:before {counter-increment: c2 0;  }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 112%;text-align: center;"><a name="bookmark0">Correlating Twitter User Sentiment and the Price of Bitcoin  via Deep Transfer Learning</a></p><p class="s2" style="padding-top: 11pt;padding-left: 159pt;text-indent: 0pt;line-height: 169%;text-align: center;">Cedric Oeldorf  i6167918 June 10, 2018</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 4pt;text-indent: 0pt;text-align: center;">Abstract</h4><p class="s3" style="padding-top: 5pt;padding-left: 29pt;text-indent: 13pt;line-height: 106%;text-align: justify;">This report investigates Deep Transfer Learning for the application on sentiment analysis. Both a feed forward neural network and a convolutional neural network were utilized for the experiment. This is followed by an investigation on whether raw sentiment correlates to the price of Bitcoin.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark1">Introduction</a></h1><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark23" class="a">Machine learning algorithms have the disadvantage of assuming the training and the testing data hold the same or identical distribution (Dai, Yang, Xue, &amp; Yu, </a>2007). If it is required to use the model on data of a different distribution, two options are available. Firstly, a new model could be trained on entirely new data. This new data would have to be sourced and should be large enough to help the model generalize. The section option, and thus the one we will discuss in this report, is to apply (deep) transfer learning. The idea is simple. Transfer learning is a technique that can circumvent the assumption of identically-distributed data by retraining the last layers of the Network using a small, specialized data set. Transfer learning is a technique that can circumvent the assumption of identically-distributed data without having to source a completely new data set. The idea behind this is to first train a Neural Network on the original data set, and then retrain the last layers of the Network using a small, specialized data set. This should fine-tune the weights to generalize better to the new data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">We find such a problem in the application domain covered by this report. When training a sentiment classification model on a corpus comprised of tweets, the algorithm struggles to ac- curately predict sentiment for more specialized tweets, such as those regarding cryptocurrencies. This comes to no surprise, as the underlying distribution between tweets regarding every day life and those regarding markets are vastly different. Extracting sentiment from tweets itself is not a menial task, especially considering the short length of the tweet, the common grammatical errors and the conversational style that characterizes twitter. This brings us to our first research question that asks whether model accuracy can be improved for a specialized tweets corpus through deep transfer learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;"><a href="#bookmark27" class="a">This brings us to our application. The cryptocurrency market is notorious for its low barriers for entry. There is no minimum investment and up until recently, many exchanges did not even request identification from customers. This results in the cryptocurrency market having a whole different set of investors comprised more of ’normal’ people as opposed to those in for example the stock market. This phenomenon saw a sharp spike in cryptocurrency market coverage on social media. Both unofficial news sources and newly invested individuals are posting their opinions and emotions regarding their new investments. Now it has been shown that the sentiment of news media has an influence on investor psychology, and thus an influence on market trends (Tetlock, </a>2007). For this reason we look to Twitter for extracting not only the sentiment of news media, but also that of traders. We hypothesize that a correlation between the average sentiment and the price of Bitcoin is present.</p></li><li style="padding-top: 4pt;padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark2">Data</a></h1><ol id="l2"><li style="padding-top: 9pt;padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark3">Modeling Data</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The main data set and the data set used for transfer learning are outlined in Table 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="509" height="1" alt="image" src="paper/Image_001.png"/></span></p><h3 style="padding-top: 7pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">Data Source Size Comment <a href="#bookmark0" class="a">Sentiment140 Stanford University 1,6 M Tweets Random (aged) tweets Stock Market Tweets Nuno Oliveira 5000 Tweets Taken off Github</a><a href="#bookmark0" class="s4">1</a></h3><p style="padding-top: 9pt;padding-left: 176pt;text-indent: 0pt;text-align: left;">Table 1: Data Sets</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Our main model will be trained on the Sentiment140 data set provided by Stanford University. This corpus is a stream of random tweets without a specific topic, nor is it tracking specific users. In other words, the corpus is not specialized for a certain domain.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark4">Application Data</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">We extracted corpus used for our application via the Twitter API. This corpus was collected from various cryptocurrency news accounts and day traders and will serve as the testing set for our hypothesis regarding correlating sentiment and the Bitcoin price. Historical Bitcoin market data was streamed from the <i>Coinmarketcap </i>API.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><span><img width="381" height="289" alt="image" src="paper/Image_002.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 145pt;text-indent: 0pt;text-align: left;">Figure 1: Sourced User Accounts</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">Figure 1 depicts 6 large accounts in terms of following. By having a large reach we assume that the news they share has a large impact on investor psychology. Together these 6 accounts have around 1.7 million followers. We also included a smaller source, <i>CryptoNewsCom</i>, as with a smaller following maybe they share more personal sentiment as opposed to large business-like accounts.</p></li></ol></li><li style="padding-top: 4pt;padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark5">Methodology</a></h1><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">This section will outline the individual elements of the pipeline used for this report. Once both</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;"><span><img width="508" height="81" alt="image" src="paper/Image_003.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 144pt;text-indent: 0pt;text-align: left;">Figure 2: Pipeline for this Report</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">data sets are brought into a format compatible with our functions, we move onto cleaning the data in preparation for embedding. The embedding process returned a vector of length 300 for each tweet. These vectors were used to train the neural network in the last step. Figure 2 depicts the pipeline as presented in the initial presentation of this report.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li style="padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark6">Preprocessing</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">In order to maximize the effectiveness of our embeddings and increase the likelihood of a successful model several preprocessing tasks were applied to the tweets.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l4"><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark7">Handling Emoji</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Before tokenization we first used Regex to find emoji and replace them with their respective sentiment.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Emoji with a positive sentiment were replaced by the word <i>EMO_POS</i>, whereas those with negative sentiment were given the replacement <i>EMO_NEG</i>. The motivation behind this is that, especially when it comes to market terms, the sentiment is incredibly unclear and usually brought across by users through the use of emoji. Figure 2 depicts two examples of both positive and negative sentiment with regards to the markets. It is clear that we would want to capture the information</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 59pt;text-indent: 0pt;text-align: left;"><span><img width="417" height="130" alt="image" src="paper/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 139pt;line-height: 208%;text-align: left;">Figure 3: Pipeline for this Report held within these emoji as opposed to erasing them.</p></li><li style="padding-top: 2pt;padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark8">User Mentions</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Many tweets include user mentions in the form of <i>@USER</i>. This could be as a reply to for example a headline, or to share an opinion about a specific user. We hypothesize that when a user include a user name, he/she feels strong enough about something to try make themselves noticed. For this reason we kept the user mention in the tweet by use of Regex to replace any user mention with the word <i>USER</i>.</p></li><li style="padding-top: 5pt;padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark9">URLs</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The Twitter landscape has changed a little over the years. Many tweets now share URLs together with an opinion about what it is being shared. Once again, we hypothesize that if a URL is accompanied by words, then this must express an opinion and could be valuable to sentiment analysis. This we replaced anything that resembles a URL with the word <i>URL</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark10">Embedding</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">In order for a machine learning algorithm to understand textual data, we have to convert the corpus into a numerical form. We make use of two popular techniques outlined below.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark11">Word2Vec</a></h3><p class="s5" style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark25" class="a">Word2Vec is a very popular word embedding technique which was developed quite recently (Mikolov, Sutskever, Chen, Corrado, &amp; Dean, </a><a href="#bookmark24" class="a">2013). It essentially trains a neural network with a single hid- den layer, but instead of using the output, the hidden layer is extracted and represents the word embedding. As it requires a lot of training data to work effectively, this report uses a pre-trained Word2Vec embedding that has been made available by Google. The model was trained on 100 billion words and has a 300-dimensional vector for around 300 words (</a>google<a href="#bookmark24" class="a">, </a><span class="p">n.d.).The motivation behind using the pre-trained net is that with our limited corpus, it would be impossible for us to achieve the word embedding quality of the one provided by Google.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark12">TF-IDF</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark26" class="a">Term Frequency - Inverse Document Frequency (TF-IDF) represents the importance of a word by adjusting the frequency of a word in a document with the frequency of the word within the entire corpus. A high TF-IDF score implies that the word has a high importance for that document (Ramos et al., </a>2003). In the next subsection we describe why we include this measure in our pipeline.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark13">Weighted Word2Vec</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The final word embedding used in this report is the Word2Vec embedding multiplied by the TF- IDF score of that word. This is motivated by the fact that tweets are very short, so certain rare words probably carry a lot of meaning but might be drowned by the sheer number of common words that appear. Multiplying the Word2Vec embedding with this score should further separate certain words in the vector space, which should improve the accuracy of our model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;text-align: left;"><span><img width="173" height="141" alt="image" src="paper/Image_005.jpg"/></span>	<span><img width="172" height="142" alt="image" src="paper/Image_006.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 81pt;text-indent: 0pt;text-align: left;">Figure 4: Compressed Embeddings of Main and Transfer Data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">Figure 3 depicts a small sample of word embeddings after having the dimensions reduced by TSNE. The fact that we are using weighted Word2Vec is most important for the transfer data set that uses many jargon words that have a lot of 7meaning in terms of sentiment. And it is the transfer set, on the right, in which we can see some separation of the sentiment classes occurring.</p></li></ol></li><li style="padding-top: 3pt;padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark14">Neural Network</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The effects of deep transfer learning will be showcased with two separate models. Firstly, a simple feed forward neural network (FFNN) and secondly, a more complex convolutional network (CNN) will be trained on the original data set and then fine-tuned through transfer learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l6"><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark15">Feed Forward Neural Network</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">In order to avoid over-fitting we kept the FFNN rather simple and applied the transfer learning on the whole network as opposed to freezing layers before training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span><img width="294" height="254" alt="image" src="paper/Image_007.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 153pt;text-indent: 0pt;text-align: left;">Figure 5: FFNN Architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">The network takes an N x 300 dimensional matrix as input, where each instance is a 300 element long vector. After having tested different hyper-parameter combinations, the final architecture took on the shape of 300 x 16 x 16 x 1 with regularizing dropout layers between each layer. This achieved an accuracy of 75 percent on an unseen section of the Sentiment140 corpus. However, as expected, it only had an accuracy of 55 percent on the specialized market corpus.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 40pt;text-indent: -35pt;text-align: left;"><h3 style="display: inline;"><a name="bookmark16">Convolutional Neural Network</a></h3><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">As opposed to the FFNN, the CNN architecture is deeper and more complex. Three 1D convo- lutional layers follow one another. These are followed by a fully connected layer and an output layer. This combination resulted in a 73 percent accuracy on the Sentiment140 corpus, which is surprisingly 2 percentage points lower than the performance of the FFNN. In the next step, when applying transfer learning, it has to be decided what layers (if any) are to be frozen for training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span><img width="362" height="147" alt="image" src="paper/Image_008.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 155pt;text-indent: 0pt;text-align: left;">Figure 6: CNN Architecture</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">Testing multiple combinations of frozen layers it became clear that there is a trade-off between the performance on the main corpus and that on the specialized corpus. An increase in accuracy on one saw a decrease in accuracy on the other. Taking this into account, it became apparent that freezing the CNN layers gave the best results. The biggest challenge, as Figure 6 depicts, is that this fine-tuning very quickly results in over fitting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span><img width="393" height="342" alt="image" src="paper/Image_009.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 147pt;text-indent: 0pt;text-align: left;">Figure 7: Training Visualization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">After only 5 epochs, the blue line, which represents the accuracy on the training set, rises very quickly to mid-90 percent accuracy. This implies severe over-fitting. Thus we had to be very careful during transfer learning not to let this happen. We tested our models on totally unseen data, so any over-fitting would depict in the results.</p></li></ol></li></ol></li><li style="padding-top: 4pt;padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark17">Model Performance</a></h1><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Performance is reported solely on the test set that stems from the market-related tweets. This is the accuracy that matters for our application on the cryptocurrency market.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l7"><li style="padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark18">Accuracy</a></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 47pt;text-indent: 0pt;text-align: left;"><span><img width="450" height="262" alt="image" src="paper/Image_010.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 115pt;text-indent: 0pt;text-align: left;">Figure 8: Transfer Learning Accuracy Increase</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Figure 8 depicts the accuracy on the testing data. The blue bars are the accuracy before transfer learning was applied, the red bars are the increase in that accuracy after transfer learning was completed. Although both the CNN and the FFNN have a very similar accuracy pre-transfer learning, the CNN outperforms the FFNN by a few percentage points. This is very interesting considering the fact that the FFNN did better on the Sentiment140 data set before transfer learning. For this reason we will focus on the CNN for our application.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 35pt;text-indent: -30pt;text-align: left;"><h2 style="display: inline;"><a name="bookmark19">Receiver Operator Characteristic</a></h2><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The Receiver Operator Characteristic curve, or ROC curve, measures the trade-off between sen- sitivity and specificity. Sensitivity measures the amount of positive predictions that are truly positive, hence it is also known as the true positive rate or recall. Specificity measures the number of negative predictions that were truly negative, also known as the true negative rate.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 89pt;text-indent: 0pt;text-align: left;"><span><img width="323" height="250" alt="image" src="paper/Image_011.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 115pt;text-indent: 0pt;text-align: left;">Figure 9: Transfer Learning Accuracy Increase</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: justify;">Seen in Figure 9, there seems to be a proportional trade-off between sensitivity and specificity. If this weren’t the case, we could have maximized one of the two properties in accordance with whichever we find more important. This can be done by adjusting the cut-off point of mapping the predicted probability to a class, which is 0.5 by default. The area under the curve (AUC) additionally shows the classification accuracy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li style="padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark20">Application</a></h1><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">For every user the sentiment over time was calculated as can be seen in Figure 10. It is interesting to note that, although noisy, there seems to be general consensus in the sentiment trend over time. Two major dips in sentiment can be identified around the dates of 2018-04-05 and 2018-05-03.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><span><img width="603" height="319" alt="image" src="paper/Image_012.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 123pt;text-indent: 0pt;text-align: left;">Figure 10: Sentiment of Individual Sources</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 14pt;text-align: left;">In order to compare the sentiment to the Bitcoin price, we average the sentiment of all sources and take a rolling mean in order to smooth the signal. The result is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span><img width="543" height="302" alt="image" src="paper/Image_013.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 112pt;text-indent: 0pt;text-align: left;">Figure 11: Average Sentiment and Bitcoin Price</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 14pt;text-align: left;">There is no clear visual correlation between the sentiment and the Bitcoin price. The Pearson- Correlation-Coefficient of 0.14 confirms this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 28pt;text-indent: -23pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark21">Conclusion</a></h1></li></ol><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Deep Transfer Learning can result in significant performance gains utilizing only a small data set without having to fully retrain a model or source a new data set. We believe that these sentiment prediction gains could further be increased with more topic related data.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Secondly there seems to be no clear cut correlation between the Bitcoin price and sentiment on twitter. We believe that when combining sentiment data with other features, a good predictive model for the Bitcoin price is possible. An additional adjustment that could be made is to use the total cryptocurrency market capitalization as opposed to just the Bitcoin price.</p><h1 style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark22">References</a></h1><p style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark23">(n.d.).</a><a name="bookmark24">&zwnj;</a></p><p style="padding-left: 29pt;text-indent: -24pt;text-align: left;"><a name="bookmark25">Dai, W., Yang, Q., Xue, G.-R., &amp; Yu, Y. (2007). Boosting for transfer learning. In </a><i>Proceedings of the 24th international conference on machine learning </i>(pp. 193–200).</p><p style="padding-left: 29pt;text-indent: -24pt;text-align: justify;"><a name="bookmark26">Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In </a><i>Advances in neural information processing systems </i>(pp. 3111–3119).</p><p style="padding-left: 2pt;text-indent: 0pt;text-align: right;"><a name="bookmark27">Ramos, J., et al. (2003). Using tf-idf to determine word relevance in document queries. In </a><i>Proceedings of the first instructional conference on machine learning </i>(Vol. 242, pp. 133–142). Tetlock, P. C. (2007). Giving content to investor sentiment: The role of media in the stock market.</p><p class="s5" style="padding-left: 29pt;text-indent: 0pt;line-height: 12pt;text-align: left;">The Journal of finance<span class="p">, </span>62 <span class="p">(3), 1139–1168.</span></p></body></html>
